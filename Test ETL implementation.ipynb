{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ETL implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook help to test our ETL implementation script.\n",
    "Main steps:\n",
    "1. Create Redshift cluster on AWS\n",
    "2. Create database\n",
    "3. Load the data from S3 to staging tables, then transform and insert data to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Redshift Cluster on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create iam and ec2 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = config['AWS']['KEY']\n",
    "SECRET = config['AWS']['SECRET']\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::172826706179:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "DWH_IAM_ROLE_NAME = config['DWH']['DWH_IAM_ROLE_NAME']\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_CLUSTER_TYPE = config['DWH']['DWH_CLUSTER_TYPE']\n",
    "DWH_NODE_TYPE = config['DWH']['DWH_NODE_TYPE']\n",
    "DWH_NUM_NODES = config['DWH']['DWH_NUM_NODES']\n",
    "DWH_CLUSTER_ZONE = config['DWH']['DWH_CLUSTER_ZONE']\n",
    "\n",
    "DWH_DB = config['CLUSTER']['DB_NAME']\n",
    "DWH_CLUSTER_IDENTIFIER = config['DWH']['DWH_CLUSTER_IDENTIFIER']\n",
    "DWH_DB_USER = config['CLUSTER']['DB_USER']\n",
    "DWH_DB_PASSWORD = config['CLUSTER']['DB_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu-central-1a'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_CLUSTER_ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#roleArn = config['IAM_ROLE']['ARN']\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        #VpcSecurityGroupIds=['vpc-5c68a336'],\n",
    "        #AvailabilityZone=DWH_CLUSTER_ZONE,\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamm\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>creating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>hiep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>lending_club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-d2f351aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key         Value\n",
       "0  ClusterIdentifier  dwhcluster  \n",
       "1  NodeType           dc2.large   \n",
       "2  ClusterStatus      creating    \n",
       "3  MasterUsername     hiep        \n",
       "4  DBName             lending_club\n",
       "5  VpcId              vpc-d2f351aa\n",
       "6  NumberOfNodes      4           "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c776efdd1579>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDWH_ENDPOINT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyClusterProps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Endpoint'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Address'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mDWH_ROLE_ARN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyClusterProps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IamRoles'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IamRoleArn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DWH_ENDPOINT :: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDWH_ENDPOINT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DWH_ROLE_ARN :: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDWH_ROLE_ARN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Endpoint'"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.c32slsdkedl6.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::172826706179:role/dwhRole\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up TCP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-fc2885b7')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "DWH_PORT = config['CLUSTER']['DB_PORT']\n",
    "\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "DROP TABLE IF EXISTS loans\n",
      "DROP TABLE IF EXISTS borrowers\n",
      "DROP TABLE IF EXISTS payment\n",
      "DROP TABLE IF EXISTS credit_history\n",
      "DROP TABLE IF EXISTS bad_debt_settlement\n",
      "DROP TABLE IF EXISTS hardship\n",
      "DROP TABLE IF EXISTS state_demo\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import psycopg2\n",
    "from sql_queries import create_table_queries, drop_table_queries\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    \"\"\"drop tables(if exists) in the database \n",
    "    :param cur: cursor object\n",
    "    :param conn: psycopg's connection object (connect to a database in Redshift cluster)\n",
    "    \n",
    "    \"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"create tables in the database\n",
    "    :param cur: cursor object\n",
    "    :param conn: psycopg's connection object (connect to a database in Redshift cluster)\n",
    "    \n",
    "    \"\"\"\n",
    "    count=0\n",
    "    for query in create_table_queries:\n",
    "        print(count)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        count+=1\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "    COPY state_demo \n",
      "    FROM 's3://lending-club-loan/transformed_data/demographic'\n",
      "    iam_role 'arn:aws:iam::172826706179:role/dwhRole'\n",
      "    format as csv;\n",
      "\n"
     ]
    },
    {
     "ename": "InternalError_",
     "evalue": "S3ServiceException:The S3 bucket addressed by the query is in a different region from this cluster.,Status 301,Error PermanentRedirect,Rid 3CD8A4343D684CDA,ExtRid ejDp7FkQZgdLcZtw9xd3HJB0zeQ1HKcH2PuRqZBXi4bH3EFTTdcMAunSYwpTP10dkrX5v/UHNVc=,CanRetry 1\nDETAIL:  \n  -----------------------------------------------\n  error:  S3ServiceException:The S3 bucket addressed by the query is in a different region from this cluster.,Status 301,Error PermanentRedirect,Rid 3CD8A4343D684CDA,ExtRid ejDp7FkQZgdLcZtw9xd3HJB0zeQ1HKcH2PuRqZBXi4bH3EFTTdcMAunSYwpTP10dkrX5v/UHNVc=,CanRetry 1\n  code:      8001\n  context:   Listing bucket=lending-club-loan prefix=transformed_data/demographic\n  query:     6152\n  location:  s3_utility.cpp:595\n  process:   padbmaster [pid=31426]\n  -----------------------------------------------\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError_\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-980e055752cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-980e055752cc>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0minsert_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-980e055752cc>\u001b[0m in \u001b[0;36minsert_tables\u001b[1;34m(cur, conn)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minsert_table_queries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError_\u001b[0m: S3ServiceException:The S3 bucket addressed by the query is in a different region from this cluster.,Status 301,Error PermanentRedirect,Rid 3CD8A4343D684CDA,ExtRid ejDp7FkQZgdLcZtw9xd3HJB0zeQ1HKcH2PuRqZBXi4bH3EFTTdcMAunSYwpTP10dkrX5v/UHNVc=,CanRetry 1\nDETAIL:  \n  -----------------------------------------------\n  error:  S3ServiceException:The S3 bucket addressed by the query is in a different region from this cluster.,Status 301,Error PermanentRedirect,Rid 3CD8A4343D684CDA,ExtRid ejDp7FkQZgdLcZtw9xd3HJB0zeQ1HKcH2PuRqZBXi4bH3EFTTdcMAunSYwpTP10dkrX5v/UHNVc=,CanRetry 1\n  code:      8001\n  context:   Listing bucket=lending-club-loan prefix=transformed_data/demographic\n  query:     6152\n  location:  s3_utility.cpp:595\n  process:   padbmaster [pid=31426]\n  -----------------------------------------------\n\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sql_queries import insert_table_queries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    \"\"\"insert data from S3 data buckets to Redshift tables.\n",
    "    :param cur: cursor object\n",
    "    :param conn: psycopg's connection object (connect to a database in Redshift cluster)\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for query in insert_table_queries:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        print(count)\n",
    "        count += 1\n",
    "        print('------------------')\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    insert_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * from borrowers LIMIT 5;')\n",
    "rows=cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 'Austin', 'Rosales', 'M', 'free')\n",
      "(18, 'Jacob', 'Rogers', 'M', 'free')\n",
      "(33, 'Bronson', 'Harris', 'M', 'free')\n",
      "(35, 'Molly', 'Taylor', 'F', 'free')\n",
      "(38, 'Gianna', 'Jones', 'F', 'free')\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * from users LIMIT 5;')\n",
    "rows=cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SOAADUU12AB0183B6F', 'Intro / Locataire (Instrumental)', 'AR70XXH1187FB44B55', 0, 101.92934)\n",
      "('SOAAFUV12AB018831D', 'Where Do The Children Play? (LP Version)', 'AR5ZGC11187FB417A3', 0, 216.05832)\n",
      "('SOAAGXT12A8C13A94D', 'Beautiful Zelda (2007 Digital Remaster)', 'ARTC1LV1187B9A4858', 1968, 145.03138)\n",
      "('SOAAKBE12A8C139075', 'Emília', 'ART1OPW1187FB3C5EF', 0, 159.7122)\n",
      "('SOAAKLA12A58A7A3CC', 'Snow Day (LP Version)', 'ARGWNT41187FB463F1', 0, 211.90485)\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * from songs LIMIT 5;')\n",
    "rows=cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AR00Y9I1187B999412', 'Akercocke', '', None, None)\n",
      "('AR01SCU1187B9A693C', 'Street Dogs', 'Boston, MA', None, None)\n",
      "('AR02T3I1187FB4D0E5', 'Aberfeldy', 'Edinburgh, Scotland', None, None)\n",
      "('AR03IZC1187FB3E058', 'Aidonia', '', None, None)\n",
      "('AR05IU31187B9B9A1A', 'Alex Ubago', 'Vitoria, Spain', 42.84751, -2.67973)\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * from artists LIMIT 5;')\n",
    "rows=cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(2018, 11, 2, 16, 35), 16, 2, 44, 11, 2018, 5)\n",
      "(datetime.datetime(2018, 11, 3, 18, 19, 10), 18, 3, 44, 11, 2018, 6)\n",
      "(datetime.datetime(2018, 11, 3, 19, 33, 39), 19, 3, 44, 11, 2018, 6)\n",
      "(datetime.datetime(2018, 11, 5, 1, 48), 1, 5, 45, 11, 2018, 1)\n",
      "(datetime.datetime(2018, 11, 5, 11, 8, 56), 11, 5, 45, 11, 2018, 1)\n",
      "(datetime.datetime(2018, 11, 5, 11, 36, 56), 11, 5, 45, 11, 2018, 1)\n",
      "(datetime.datetime(2018, 11, 5, 18, 26, 7), 18, 5, 45, 11, 2018, 1)\n",
      "(datetime.datetime(2018, 11, 6, 8, 49, 19), 8, 6, 45, 11, 2018, 2)\n",
      "(datetime.datetime(2018, 11, 6, 16, 38, 15), 16, 6, 45, 11, 2018, 2)\n",
      "(datetime.datetime(2018, 11, 6, 20, 12, 11), 20, 6, 45, 11, 2018, 2)\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * from time LIMIT 10;')\n",
    "rows=cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DWH_CLUSTER_IDENTIFIER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-aaba5d1cafe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mredshift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mClusterIdentifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDWH_CLUSTER_IDENTIFIER\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mSkipFinalClusterSnapshot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'DWH_CLUSTER_IDENTIFIER' is not defined"
     ]
    }
   ],
   "source": [
    "redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'hiep',\n",
       "  'DBName': 'lending_club',\n",
       "  'Endpoint': {'Address': 'dwhcluster.c32slsdkedl6.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2020, 4, 10, 12, 14, 9, 516000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-fc2885b7',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-d2f351aa',\n",
       "  'AvailabilityZone': 'us-west-2a',\n",
       "  'PreferredMaintenanceWindow': 'thu:08:00-thu:08:30',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::172826706179:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2020, 4, 16, 8, 0, tzinfo=tzutc())},\n",
       " 'ResponseMetadata': {'RequestId': 'b5a70d20-7b77-11ea-9ba0-a3f2fc9c1e68',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b5a70d20-7b77-11ea-9ba0-a3f2fc9c1e68',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2388',\n",
       "   'vary': 'Accept-Encoding',\n",
       "   'date': 'Fri, 10 Apr 2020 22:07:45 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift.delete_cluster(ClusterIdentifier='dwhCluster',  SkipFinalClusterSnapshot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
